{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwc-W6vKwPi7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DownloadConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Log in to Hugging Face Hub using your token\n",
        "login(token=hf_token)\n",
        "\n",
        "# Create a DownloadConfig object with the authentication token\n",
        "download_config = DownloadConfig()\n",
        "\n",
        "# Loading only 100 samples, passing the download_config to load_dataset\n",
        "cv = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"train[:30]\", download_config=download_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXBucrHyXWbI",
        "outputId": "05b62521-bcf9-43df-f9bc-349ef8921a4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 30/30 [00:13<00:00,  2.26 examples/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram, Resample\n",
        "\n",
        "mel_transform = MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "\n",
        "def preprocess(batch):\n",
        "    speech_array = batch[\"audio\"][\"array\"]\n",
        "    orig_sample_rate = batch[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "    # Convert to float32 tensor directly (not float64!)\n",
        "    waveform = torch.tensor(speech_array, dtype=torch.float32).unsqueeze(0)  # shape: (1, num_samples)\n",
        "\n",
        "    # Resample to 16kHz\n",
        "    resampler = Resample(orig_freq=orig_sample_rate, new_freq=16000)\n",
        "    audio_resampled = resampler(waveform)\n",
        "\n",
        "    # Compute mel spectrogram\n",
        "    mel_spec = mel_transform(audio_resampled).squeeze(0).transpose(0, 1)  # shape: (time, mel)\n",
        "\n",
        "    # Add fields to the batch\n",
        "    batch[\"input\"] = mel_spec\n",
        "    batch[\"target\"] = batch[\"sentence\"].lower()\n",
        "    return batch\n",
        "\n",
        "# Apply preprocessing\n",
        "cv = cv.map(preprocess)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l75II-ocZLKk"
      },
      "outputs": [],
      "source": [
        "# Build vocab from dataset\n",
        "vocab = list(\"abcdefghijklmnopqrstuvwxyz '\")\n",
        "char2idx = {c: i for i, c in enumerate(vocab)}\n",
        "blank_idx = len(vocab)\n",
        "\n",
        "def encode_text(text):\n",
        "    return [char2idx[c] for c in text if c in char2idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ4qZHyqXag1"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN_ASRModel(nn.Module):\n",
        "    def __init__(self, input_dim=128, hidden_dim=256, output_dim=29):  # 28 chars + blank\n",
        "        super(CNN_ASRModel, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, hidden_dim * 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # [B, T, F] -> [B, F, T]\n",
        "        x = self.cnn(x)\n",
        "        x = x.permute(0, 2, 1)  # [B, F, T] -> [B, T, F]\n",
        "        x = self.fc(x)\n",
        "        return x.log_softmax(dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq7cJgvqZU0U"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Convert inputs to tensors if they aren't already\n",
        "    inputs = [torch.tensor(b[\"input\"], dtype=torch.float32) if isinstance(b[\"input\"], list) else b[\"input\"] for b in batch]\n",
        "    targets = [torch.tensor(encode_text(b[\"target\"]), dtype=torch.int32) for b in batch]\n",
        "\n",
        "    input_lengths = [i.shape[0] for i in inputs]\n",
        "    target_lengths = [len(t) for t in targets]\n",
        "\n",
        "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
        "    targets_concatenated = torch.cat(targets)\n",
        "\n",
        "    return inputs_padded, targets_concatenated, torch.tensor(input_lengths), torch.tensor(target_lengths)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(cv, batch_size=2, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgUvQDBpZZs0",
        "outputId": "ac00883d-2187-430f-bd8f-9de8d82b12f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 401.9208\n",
            "Epoch 11, Loss: 47.0652\n",
            "Epoch 21, Loss: 46.3440\n",
            "Epoch 31, Loss: 45.3573\n",
            "Epoch 41, Loss: 44.5494\n",
            "Epoch 51, Loss: 43.2141\n",
            "Epoch 61, Loss: 41.7673\n",
            "Epoch 71, Loss: 38.8777\n",
            "Epoch 81, Loss: 35.7319\n",
            "Epoch 91, Loss: 33.4455\n",
            "Epoch 101, Loss: 32.9738\n",
            "Epoch 111, Loss: 26.1256\n",
            "Epoch 121, Loss: 27.7236\n",
            "Epoch 131, Loss: 21.9867\n",
            "Epoch 141, Loss: 20.4134\n",
            "Epoch 151, Loss: 18.3801\n",
            "Epoch 161, Loss: 26.4442\n",
            "Epoch 171, Loss: 17.7505\n",
            "Epoch 181, Loss: 17.6687\n",
            "Epoch 191, Loss: 15.9362\n"
          ]
        }
      ],
      "source": [
        "model = CNN_ASRModel()\n",
        "criterion = nn.CTCLoss(blank=blank_idx, zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    total_loss = 0\n",
        "    for inputs, targets, input_lens, target_lens in dataloader:\n",
        "        logits = model(inputs)  # [B, T, C]\n",
        "        log_probs = logits.permute(1, 0, 2)  # [T, B, C] for CTC\n",
        "        loss = criterion(log_probs, targets, input_lens, target_lens)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    if epoch%10==0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-j1pYbLO9qW"
      },
      "outputs": [],
      "source": [
        "vocab.append(' ')\n",
        "blank_idx = len(vocab) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmZMQu3yO9qW",
        "outputId": "6c75b5c4-aa09-4bdc-f2fc-ea7c22f43fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: ts device has a cathode iside an as\n",
            "Actual   : this device has a cathode inside an anode wire cage.\n"
          ]
        }
      ],
      "source": [
        "def greedy_decoder(logits):\n",
        "    pred_ids = logits.argmax(dim=-1)\n",
        "    decoded = []\n",
        "    for pred in pred_ids:\n",
        "        prev = -1\n",
        "        text = ''\n",
        "        for p in pred:\n",
        "            p = p.item()\n",
        "            if p != prev and p != blank_idx:\n",
        "                text += vocab[p]\n",
        "            prev = p\n",
        "        decoded.append(text)\n",
        "    return decoded\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample = cv[0]\n",
        "\n",
        "\n",
        "    input_tensor = torch.tensor(sample[\"input\"], dtype=torch.float32).unsqueeze(0)  # [1, T, F]\n",
        "\n",
        "    output = model(input_tensor)\n",
        "    prediction = greedy_decoder(output)\n",
        "\n",
        "    print(\"Predicted:\", prediction[0])\n",
        "    print(\"Actual   :\", sample[\"target\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6-IzKolO9qW",
        "outputId": "551f5ed1-bb78-45c9-fdee-89eb7842a749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', \"'\"]\n"
          ]
        }
      ],
      "source": [
        "print(vocab)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}